{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M4 Dataset Benchmark Code\n",
    "> Generic code to experiment and produce the final benchmark.py codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# ml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# local\n",
    "from models.benchmark import NaivePredictor\n",
    "from models.cnn import SimpleCNN\n",
    "from utils.plot import plot_predictions\n",
    "from experiment import Experiment\n",
    "from utils.m4 import smape, mase, M4DatasetGenerator\n",
    "from utils.ml import print_num_weights\n",
    "#\n",
    "from neuralforecast.core import NeuralForecast\n",
    "from neuralforecast.models import Informer, Autoformer, FEDformer, PatchTST\n",
    "from neuralforecast.models import NBEATS, NHITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m run_sp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDaily\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekly\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonthly\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuarterly\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearly\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m prediction_frequency \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHourly\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekly\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m,}[run_sp]\n\u001b[0;32m----> 7\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(model_name, model_conf):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcnn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "model_name = 'Informer'\n",
    "run_sp = 'Weekly'\n",
    "\n",
    "assert run_sp in ['Hourly','Daily','Weekly','Monthly','Quarterly','Yearly']\n",
    "prediction_frequency = {'Hourly':'h','Weekly':'W',}[run_sp]\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "def get_model(model_name, model_conf):\n",
    "    if model_name == 'cnn':\n",
    "        return SimpleCNN(model_conf['input_size'], 64)\n",
    "    elif model_name == 'naive':\n",
    "        return NaivePredictor()\n",
    "    elif model_name == 'NBEATS':\n",
    "        return NBEATS(input_size=2 * model_conf['input_size'], h=model_conf['forecasting_horizon'], max_steps=50)\n",
    "    elif model_name == 'NHITS':\n",
    "        return NHITS(input_size=2 * model_conf['input_size'], h=model_conf['forecasting_horizon'], max_steps=50)\n",
    "    elif model_name == 'Informer':\n",
    "        return Informer(\n",
    "                hidden_size=32, n_head=4, conv_hidden_size=8, encoder_layers= 2, decoder_layers=1,\n",
    "                input_size=model_conf['input_size'], # Input size\n",
    "                h=model_conf['forecasting_horizon'], # Forecasting horizon\n",
    "                max_steps=100, # Number of training iterations\n",
    "                batch_size=1024, learning_rate=0.01, )\n",
    "\n",
    "\n",
    "metrics_table = {'serie_id':[],'smape':[],'mase':[],}\n",
    "smape_list, mase_list = [], []\n",
    "m4_data = M4DatasetGenerator([run_sp])\n",
    "num_of_series = m4_data.data_dict[run_sp]['num']\n",
    "block_size = m4_data.data_dict[run_sp]['fh']\n",
    "fh = m4_data.data_dict[run_sp]['fh']\n",
    "\n",
    "\n",
    "#\n",
    "for train_serie, test_serie, serie_id, fh, freq, serie_sp in m4_data.generate(random=True):\n",
    "    assert fh == block_size\n",
    "    # print(f'Serie {serie_id} ({run_sp}) Train serie: {len(train_serie)} steps - Test serie: {len(test_serie)} steps | fh={fh} | freq={freq}')\n",
    "    # synthetic days\n",
    "    train_daterange = pd.date_range(start='1980', periods=len(train_serie), freq=prediction_frequency)\n",
    "    test_daterange = pd.date_range(start=train_daterange[-1], periods=len(test_serie)+1, freq=prediction_frequency)[1:] # len + 1 because the first day is on train dates\n",
    "    #\n",
    "    model_conf = {}\n",
    "    model_conf['input_size'] = min(fh*4, len(train_serie)//10)\n",
    "    model_conf['forecasting_horizon'] = fh\n",
    "\n",
    "    model = get_model(model_name, model_conf)\n",
    "    print_num_weights(model)\n",
    "\n",
    "    if model_name in ['Informer','autoformer','fedformer','patchtst','NHITS']:\n",
    "        nf = NeuralForecast(models=[model], freq=prediction_frequency, local_scaler_type='standard')\n",
    "        train_df = pd.DataFrame({\n",
    "            'unique_id':serie_id,\n",
    "            'y':train_serie, \n",
    "            'ds':train_daterange\n",
    "            })\n",
    "        val_size = 0#int(.2 * len(train_serie)) # 20% for validation\n",
    "        # model train\n",
    "        nf.fit(df=train_df, val_size=val_size, verbose=False)\n",
    "        pred_y = nf.predict()\n",
    "        #\n",
    "        assert all(pred_y.ds == test_daterange) # check \n",
    "        pred_y = pred_y[model_name].values\n",
    "    else:\n",
    "        exp_conf = { 'model': model, 'model_n_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad), \n",
    "                    'input_len':block_size, 'forecast_horizon':fh, 'feature_dim':1,\n",
    "                    'frequency':serie_sp.lower(),\n",
    "                    'scaler':MinMaxScaler((-1,1)),\n",
    "                    'decompose': False, #detrend and de-sazonalize\n",
    "                    'freq':freq, 'device':'cuda', 'verbose':False,}\n",
    "        train_conf = {\n",
    "            'epochs':100,\n",
    "            'lr': 1e-3, \n",
    "            'batch_size':1024,\n",
    "            'validate_freq':10,\n",
    "            'verbose':False, # stop training if loss dont decrease 0.5% 5 consecutive steps\n",
    "            # 'early_stop':EarlyStopperPercent(patience=5, min_percent=0.005, verbose=False),\n",
    "        }\n",
    "        exp = Experiment(exp_conf)\n",
    "        exp.set_dataset(linear_serie=train_serie, train=True)\n",
    "        exp.train(train_conf)\n",
    "        # test\n",
    "        last_train_values = train_serie[-block_size:]\n",
    "        pred_y = exp.predict(last_train_values, fh)\n",
    "    #\n",
    "    #\n",
    "    # check if negative or extreme (M4)\n",
    "    pred_y[pred_y < 0] = 0\n",
    "    pred_y[pred_y > (1000 * np.max(train_serie))] = np.max(train_serie)\n",
    "\n",
    "    # Metrics\n",
    "    metrics_table['serie_id'].append(serie_id)\n",
    "    metrics_table['smape'].append(smape(test_serie, pred_y)*100)\n",
    "    metrics_table['mase'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "    print(f'Serie {serie_id}-{serie_sp} Finished')\n",
    "    print(test_serie[:10])\n",
    "    print(pred_y[:10])\n",
    "    plot_predictions(train_serie, test_serie, pred_y)\n",
    "    \n",
    "#\n",
    "metrics_dict = {\n",
    "    'smape_mean': np.round(np.mean(metrics_table['smape'], dtype=float), 3), \n",
    "    'mase_mean':  np.round(np.mean(metrics_table['mase'], dtype=float), 3),\n",
    "    #\n",
    "    'smape_std':  np.round(np.std(metrics_table['smape'], dtype=float), 3),\n",
    "    'mase_std':   np.round(np.std(metrics_table['mase'], dtype=float), 3),\n",
    "}\n",
    "print(f'''\n",
    "    Experiment Finished\n",
    "''')\n",
    "for k, v in metrics_dict.items(): print(f'      {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2146854825.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 27\u001b[0;36m\u001b[0m\n\u001b[0;31m    scaler = None MinMaxScaler((-1,1))\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model_name = 'naive'\n",
    "run_sp = 'Weekly'\n",
    "m4_data = M4DatasetGenerator([run_sp])\n",
    "\n",
    "def get_model(model_name, model_conf):\n",
    "    if model_name == 'naive':\n",
    "        return NaivePredictor()\n",
    "    \n",
    "np.random.seed(123)\n",
    "#\n",
    "# Inicializations\n",
    "#\n",
    "metrics_table = {'serie_id':[],'smape':[],'mase':[],}\n",
    "smape_list, mase_list = [], []\n",
    "num_of_series = m4_data.data_dict[run_sp]['num']\n",
    "block_size = m4_data.data_dict[run_sp]['fh']\n",
    "fh = m4_data.data_dict[run_sp]['fh']\n",
    "#\n",
    "# Model Hiperparams\n",
    "#\n",
    "if model_name == 'naive':\n",
    "    scaler = None\n",
    "    decompose = False\n",
    "    epochs = 1\n",
    "    batch_size = 0\n",
    "else:\n",
    "    scaler = None #MinMaxScaler((-1,1))\n",
    "    # d_model = 64\n",
    "    # batch_size = 512 #512\n",
    "    # epochs = 128\n",
    "\n",
    "MAX_SERIES = 10**8\n",
    "for train_serie, test_serie, serie_id, fh, freq, serie_sp in m4_data.generate(n_series=MAX_SERIES, random=False):\n",
    "    assert fh == block_size\n",
    "    model_conf = {\n",
    "        'input_size':fh,\n",
    "        'h':fh\n",
    "    }\n",
    "    \n",
    "    model = get_model(model_name, model_conf) #\n",
    "    \n",
    "    exp_conf = {\n",
    "            # Model\n",
    "            'model': model,\n",
    "            'model_n_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad), \n",
    "            'input_len':block_size,\n",
    "            'forecast_horizon':fh,\n",
    "            'feature_dim':1,\n",
    "            # Data\n",
    "            'frequency':serie_sp.lower(),\n",
    "            'scaler':scaler,\n",
    "            'decompose': decompose, #detrend and de-sazonalize\n",
    "            'freq':freq,\n",
    "            # Others\n",
    "            'device':'cuda',\n",
    "            'verbose':False,\n",
    "    }\n",
    "    train_conf = {\n",
    "        'epochs':epochs,\n",
    "        'lr':lr, \n",
    "        'batch_size':batch_size,\n",
    "        'validate_freq':10,\n",
    "        'verbose':False, # stop training if loss dont decrease 0.5% 5 consecutive steps\n",
    "        'early_stop':EarlyStopperPercent(patience=5, min_percent=0.005, verbose=False),\n",
    "    }\n",
    "    if model_name == 'decoder_transformer':\n",
    "        print(serie_id)\n",
    "        train_x = torch.tensor(scaler.fit_transform(train_serie.reshape(-1, 1)).reshape(-1), dtype=torch.float32)\n",
    "\n",
    "        x, y, m = get_x_y(train_x, block_size=512)\n",
    "        train_conf['train_dataset'] = DecoderDataset(x.unsqueeze(-1), y.unsqueeze(-1), m)\n",
    "        model.fit(train_conf)\n",
    "\n",
    "        train_x = train_x.to('cuda').view(1, -1, 1)\n",
    "        pred_y = model.predict(train_x, len(test_serie)).cpu().numpy()\n",
    "        pred_y = scaler.inverse_transform(pred_y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        exp = Experiment(exp_conf)\n",
    "        exp.set_dataset(linear_serie=train_serie, train=True)\n",
    "        # exp.set_dataset(linear_serie=test_serie)\n",
    "        exp.train(train_conf)\n",
    "        # test\n",
    "        last_train_values = train_serie[-block_size:]\n",
    "        pred_y = exp.predict(last_train_values, fh)\n",
    "    \n",
    "    # check if negative or extreme (M4)\n",
    "    pred_y[pred_y < 0] = 0\n",
    "    pred_y[pred_y > (1000 * np.max(train_serie))] = np.max(train_serie)\n",
    "\n",
    "    # Metrics\n",
    "    metrics_table['serie_id'].append(serie_id)\n",
    "    metrics_table['smape'].append(smape(test_serie, pred_y)*100)\n",
    "    metrics_table['mase'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "    print(f'Serie {serie_id}-{serie_sp} Finished')\n",
    "    plot_predictions(train_serie, test_serie, pred_y)\n",
    "    \n",
    "#\n",
    "metrics_dict = {\n",
    "    'smape_mean': np.round(np.mean(metrics_table['smape'], dtype=float), 3), \n",
    "    'mase_mean':  np.round(np.mean(metrics_table['mase'], dtype=float), 3),\n",
    "    #\n",
    "    'smape_std':  np.round(np.std(metrics_table['smape'], dtype=float), 3),\n",
    "    'mase_std':   np.round(np.std(metrics_table['mase'], dtype=float), 3),\n",
    "}\n",
    "print(f'''\n",
    "    Experiment Finished\n",
    "''')\n",
    "for k, v in metrics_dict.items(): print(f'      {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading M4 Data...\n",
      "Loaded:\n",
      "    => Weekly has 359 series\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.80 GiB of which 26.75 MiB is free. Process 16155 has 5.09 GiB memory in use. Including non-PyTorch memory, this process has 82.00 MiB memory in use. Of the allocated memory 1.50 KiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 82\u001b[0m\n\u001b[1;32m     77\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39minverse_transform(pred_y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 82\u001b[0m     exp \u001b[38;5;241m=\u001b[39m \u001b[43mExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_conf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     exp\u001b[38;5;241m.\u001b[39mset_dataset(linear_serie\u001b[38;5;241m=\u001b[39mtrain_serie, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;66;03m# exp.set_dataset(linear_serie=test_serie)\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/experiment.py:35\u001b[0m, in \u001b[0;36mExperiment.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m     31\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/lightning_fabric/utilities/device_dtype_mixin.py:55\u001b[0m, in \u001b[0;36m_DeviceDtypeModuleMixin.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m device, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     54\u001b[0m _update_properties(\u001b[38;5;28mself\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:849\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, buf \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 849\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffers[key] \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 5.80 GiB of which 26.75 MiB is free. Process 16155 has 5.09 GiB memory in use. Including non-PyTorch memory, this process has 82.00 MiB memory in use. Of the allocated memory 1.50 KiB is allocated by PyTorch, and 2.00 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading M4 Data...\n",
      "Loaded:\n",
      "    => Weekly has 359 series\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_serie, test_serie, serie_id, fh, freq, serie_sp \u001b[38;5;129;01min\u001b[39;00m m4_data\u001b[38;5;241m.\u001b[39mgenerate(n_series\u001b[38;5;241m=\u001b[39mMAX_SERIES, random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m fh \u001b[38;5;241m==\u001b[39m block_size\n\u001b[0;32m---> 74\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_conf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     exp_conf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     77\u001b[0m             \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     91\u001b[0m     }\n\u001b[1;32m     92\u001b[0m     train_conf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m:epochs,\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m:lr, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stop\u001b[39m\u001b[38;5;124m'\u001b[39m:EarlyStopperPercent(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_percent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     99\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_name, model_conf)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NaivePredictor()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvanilla_transformer\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[43mVanillaTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_conf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: VanillaTransformer.__init__() missing 1 required positional argument: 'input_size'"
     ]
    }
   ],
   "source": [
    "model_name = 'vanilla_transformer' # ['vanilla_transformer','cnn','decoder_transformer'][2]\n",
    "run_sp = 'Weekly'\n",
    "\n",
    "assert(model_name in ['cnn','naive', 'vanilla_transformer','decoder_transformer'])\n",
    "assert run_sp in ['Hourly','Daily','Weekly','Monthly','Quarterly','Yearly']\n",
    "m4_data = M4DatasetGenerator([run_sp])\n",
    "\n",
    "def get_model(model_name, model_conf):\n",
    "    if model_name == 'cnn':\n",
    "        return SimpleCNN(model_conf['block_size'], model_conf['d_model'])\n",
    "    elif model_name == 'naive':\n",
    "        return NaivePredictor()\n",
    "    elif model_name == 'vanilla_transformer':\n",
    "        return  VanillaTransformer(model_conf)\n",
    "if TRACK:\n",
    "    mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "    mlflow.set_experiment(f\"M4Benchmark {model_name}\")\n",
    "    mlflow.set_experiment_tag('model', model_name)\n",
    "\n",
    "np.random.seed(123)\n",
    "#\n",
    "# Inicializations\n",
    "#\n",
    "metrics_table = {'serie_id':[],'smape':[],'mase':[],}\n",
    "smape_list, mase_list = [], []\n",
    "num_of_series = m4_data.data_dict[run_sp]['num']\n",
    "block_size = m4_data.data_dict[run_sp]['fh']\n",
    "fh = m4_data.data_dict[run_sp]['fh']\n",
    "#\n",
    "# Model Hiperparams\n",
    "#\n",
    "d_model = 64\n",
    "batch_size = 512 #512\n",
    "epochs = 128\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "decompose = False\n",
    "MAX_SERIES = 10\n",
    "#\n",
    "# model_conf = {'block_size':block_size, 'd_model':d_model}\n",
    "if model_name == 'vanilla_transformer':\n",
    "    model_conf = {'block_size':block_size, 'd_model': 16, 'num_heads': 2, 'num_layers': 2,'dim_feedforward':128,'device':'cuda'}\n",
    "    lr = 1e-4\n",
    "elif model_name == 'decoder_transformer':\n",
    "    model_conf = {\n",
    "    'd_model': 32, \n",
    "    'num_heads': 4, \n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward':128,\n",
    "    'block_size':512,\n",
    "    'device':'cuda',\n",
    "    'pad_token':-20\n",
    "}\n",
    "    lr = 1e-4\n",
    "else:\n",
    "    lr = 1e-3\n",
    "\n",
    "#\n",
    "if TRACK:\n",
    "    mlflow.start_run(run_name=f'{run_sp}')\n",
    "    mlflow.log_param('model_name', model_name)\n",
    "    mlflow.log_param('d_model', d_model)\n",
    "    mlflow.log_param('block_size', block_size)\n",
    "    mlflow.log_param('forecast_horizon', fh)\n",
    "    mlflow.log_param('decompose', decompose)\n",
    "\n",
    "    mlflow.log_param('series', num_of_series)\n",
    "    mlflow.log_param('scaler', scaler)\n",
    "    mlflow.log_param('batch_size', batch_size)\n",
    "    mlflow.log_param('epochs', epochs)\n",
    "    mlflow.log_param('lr', lr)\n",
    "    \n",
    "for train_serie, test_serie, serie_id, fh, freq, serie_sp in m4_data.generate(n_series=MAX_SERIES, random=True):\n",
    "    assert fh == block_size\n",
    "    model = get_model(model_name, model_conf) #\n",
    "    \n",
    "    exp_conf = {\n",
    "            # Model\n",
    "            'model': model,\n",
    "            'model_n_parameters': sum(p.numel() for p in model.parameters() if p.requires_grad), \n",
    "            'input_len':block_size,\n",
    "            'forecast_horizon':fh,\n",
    "            'feature_dim':1,\n",
    "            # Data\n",
    "            'frequency':serie_sp.lower(),\n",
    "            'scaler':scaler,\n",
    "            'decompose': decompose, #detrend and de-sazonalize\n",
    "            'freq':freq,\n",
    "            # Others\n",
    "            'device':'cuda',\n",
    "            'verbose':False,\n",
    "    }\n",
    "    train_conf = {\n",
    "        'epochs':epochs,\n",
    "        'lr':lr, \n",
    "        'batch_size':batch_size,\n",
    "        'validate_freq':10,\n",
    "        'verbose':False, # stop training if loss dont decrease 0.5% 5 consecutive steps\n",
    "        'early_stop':EarlyStopperPercent(patience=5, min_percent=0.005, verbose=False),\n",
    "    }\n",
    "    if model_name == 'decoder_transformer':\n",
    "        print(serie_id)\n",
    "        train_x = torch.tensor(scaler.fit_transform(train_serie.reshape(-1, 1)).reshape(-1), dtype=torch.float32)\n",
    "\n",
    "        x, y, m = get_x_y(train_x, block_size=512)\n",
    "        train_conf['train_dataset'] = DecoderDataset(x.unsqueeze(-1), y.unsqueeze(-1), m)\n",
    "        model.fit(train_conf)\n",
    "\n",
    "        train_x = train_x.to('cuda').view(1, -1, 1)\n",
    "        pred_y = model.predict(train_x, len(test_serie)).cpu().numpy()\n",
    "        pred_y = scaler.inverse_transform(pred_y.reshape(-1,1)).reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        exp = Experiment(exp_conf)\n",
    "        exp.set_dataset(linear_serie=train_serie, train=True)\n",
    "        # exp.set_dataset(linear_serie=test_serie)\n",
    "        exp.train(train_conf)\n",
    "        # test\n",
    "        last_train_values = train_serie[-block_size:]\n",
    "        pred_y = exp.predict(last_train_values, fh)\n",
    "    \n",
    "    # check if negative or extreme (M4)\n",
    "    pred_y[pred_y < 0] = 0\n",
    "    pred_y[pred_y > (1000 * np.max(train_serie))] = np.max(train_serie)\n",
    "\n",
    "    # Metrics\n",
    "    metrics_table['serie_id'].append(serie_id)\n",
    "    metrics_table['smape'].append(smape(test_serie, pred_y)*100)\n",
    "    metrics_table['mase'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "    print(f'Serie {serie_id}-{serie_sp} Finished')\n",
    "    plot_predictions(train_serie, test_serie, pred_y)\n",
    "    \n",
    "#\n",
    "metrics_dict = {\n",
    "    'smape_mean': np.round(np.mean(metrics_table['smape'], dtype=float), 3), \n",
    "    'mase_mean':  np.round(np.mean(metrics_table['mase'], dtype=float), 3),\n",
    "    #\n",
    "    'smape_std':  np.round(np.std(metrics_table['smape'], dtype=float), 3),\n",
    "    'mase_std':   np.round(np.std(metrics_table['mase'], dtype=float), 3),\n",
    "}\n",
    "if TRACK:\n",
    "    mlflow.log_metrics(metrics_dict)\n",
    "    mlflow.log_table(metrics_table, artifact_file='metrics_table')\n",
    "\n",
    "print(f'''\n",
    "    Experiment Finished\n",
    "''')\n",
    "for k, v in metrics_dict.items(): print(f'      {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serie_id</th>\n",
       "      <th>smape</th>\n",
       "      <th>mase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>W1</td>\n",
       "      <td>14.393604</td>\n",
       "      <td>74.858755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>W2</td>\n",
       "      <td>11.724994</td>\n",
       "      <td>32.572269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  serie_id      smape       mase\n",
       "0       W1  14.393604  74.858755\n",
       "1       W2  11.724994  32.572269"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Experiment Finished\n",
      "\n",
      "      smape_mean: 13.059\n",
      "      mase_mean: 53.716\n",
      "      smape_std: 1.334\n",
      "      mase_std: 21.143\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import json\n",
    "metrics_table = json.load(open('results/Weekly_Informer_metrics_table.json'))\n",
    "display(pd.DataFrame(metrics_table).head(10))\n",
    "\n",
    "metrics_dict = {\n",
    "    'smape_mean': np.round(np.mean(metrics_table['smape'], dtype=float), 3), \n",
    "    'mase_mean':  np.round(np.mean(metrics_table['mase'], dtype=float), 3),\n",
    "    #\n",
    "    'smape_std':  np.round(np.std(metrics_table['smape'], dtype=float), 3),\n",
    "    'mase_std':   np.round(np.std(metrics_table['mase'], dtype=float), 3),\n",
    "}\n",
    "print(f'''\n",
    "    Experiment Finished\n",
    "''')\n",
    "for k, v in metrics_dict.items(): print(f'      {k}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "    Experiment Finished\n",
    "''')\n",
    "for k, v in metrics_dict.items(): print(f'      {k}: {v}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
