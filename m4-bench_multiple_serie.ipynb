{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## M4-Competition Benchmark Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler, MinMaxScaler\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mm4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesTransformer\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/models/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# from benchmark import *\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from utils.m4 import *\n",
    "from models import TimeSeriesTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('./M4-methods/Dataset/M4-info.csv')\n",
    "data_dict = {}\n",
    "forecast_horizon = {\n",
    "    'Hourly':48,\n",
    "    'Daily':14,\n",
    "    'Weekly':13,\n",
    "    'Monthly':18,\n",
    "    'Quarterly':8,\n",
    "    'Yearly':6,\n",
    "}\n",
    "frequency = {\n",
    "    'Hourly':24,\n",
    "    'Daily':1,\n",
    "    'Weekly':1,\n",
    "    'Monthly':12,\n",
    "    'Quarterly':4,\n",
    "    'Yearly':1,\n",
    "}\n",
    "\n",
    "USED_PERIODS = ['Weekly'] #['Hourly','Daily','Weekly','Monthly','Quarterly','Yearly']\n",
    "\n",
    "for SP in USED_PERIODS:\n",
    "    data_dict[SP] = {\n",
    "        'train':pd.read_csv(f'M4-methods/Dataset/Train/{SP}-train.csv'),\n",
    "        'test': pd.read_csv(f'M4-methods/Dataset/Test/{SP}-test.csv')\n",
    "    }\n",
    "df_info = df_info[df_info['SP'].isin(USED_PERIODS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Loop (Single Serie Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for serie_index in tqdm(range(len(df_info))):\n",
    "    serie_info = df_info.iloc[serie_index]\n",
    "    serie_id = serie_info.M4id\n",
    "    serie_sp = serie_info.SP\n",
    "    fh = forecast_horizon[serie_sp]\n",
    "    freq = frequency[serie_sp]\n",
    "    in_size = 3  # number of points used as input for each forecast\n",
    "    #\n",
    "    train_df = data_dict[serie_sp]['train']\n",
    "    # filtra a serie, remove os valores nan e remove o id da serie(presente no 1 valor)\n",
    "    ts = train_df[train_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "    ts = np.asarray(ts, dtype=np.float32)\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # remove seasonality\n",
    "    seasonality_in = deseasonalize(ts, freq)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "    # detrending\n",
    "    a, b = detrend(ts)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] - ((a * i) + b)\n",
    "    # scaler = MinMaxScaler()\n",
    "    # scaler.fit(ts[:-fh].reshape(-1, 1))\n",
    "    # ts = scaler.transform(ts.reshape(-1, 1)).flatten()\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    # y_hat_test_MLP = mlp_bench(x_train.copy(), y_train.copy(), x_test.copy(), fh)\n",
    "\n",
    "    # ts = scaler.inverse_transform(ts.reshape(-1, 1)).flatten()\n",
    "    # y_hat_test_MLP = scaler.inverse_transform(y_hat_test_MLP.reshape(-1, 1)).flatten()\n",
    "    # y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    y_hat_test_MLP = mlp_bench(x_train, y_train, x_test, fh)\n",
    "    for i in range(0, 29):\n",
    "        y_hat_test_MLP = np.vstack((y_hat_test_MLP, mlp_bench(x_train, y_train, x_test, fh)))\n",
    "    y_hat_test_MLP = np.median(y_hat_test_MLP, axis=0)\n",
    "\n",
    "    # add trend\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] + ((a * i) + b)\n",
    "\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test_MLP[i] = y_hat_test_MLP[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "\n",
    "    # add seasonality\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    for i in range(len(ts), len(ts) + fh):\n",
    "        y_hat_test_MLP[i - len(ts)] = y_hat_test_MLP[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    # check if negative or extreme\n",
    "    for i in range(len(y_hat_test_MLP)):\n",
    "        if y_hat_test_MLP[i] < 0:\n",
    "            y_hat_test_MLP[i] = 0\n",
    "            \n",
    "        if y_hat_test_MLP[i] > (1000 * max(ts)):\n",
    "            y_hat_test_MLP[i] = max(ts)   \n",
    "\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "\n",
    "    \n",
    "    # Create Train examples for MLP and RNN\n",
    "   \n",
    "    # train_x, train_y, test_x, test_y = split_into_train_test(train_serie, test_serie, in_size)\n",
    "    \n",
    "    #Naive Forecast (Benchmark)\n",
    "    \n",
    "    # pred_y =  naive_predict(train_serie, test_serie, fh)\n",
    "    # ERRORS['naive_1'][serie_sp]['sMAPE'].append(smape(test_serie, pred_y))\n",
    "    # ERRORS['naive_1'][serie_sp]['MASE'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "    \n",
    "    # MLP (Benchmark)\n",
    "    #\n",
    "    ERRORS['mlp'][serie_sp]['sMAPE'].append(smape(y_test, y_hat_test_MLP))\n",
    "    ERRORS['mlp'][serie_sp]['MASE'].append(mase(ts[:-fh], y_test, y_hat_test_MLP, freq))  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERRORS = {\n",
    "    'naive_1':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS},\n",
    "    'mlp':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS}\n",
    "    }\n",
    "\n",
    "for serie_index in tqdm(range(len(df_info))):\n",
    "    serie_info = df_info.iloc[serie_index]\n",
    "    serie_id = serie_info.M4id\n",
    "    serie_sp = serie_info.SP\n",
    "    fh = forecast_horizon[serie_sp]\n",
    "    freq = frequency[serie_sp]\n",
    "    in_size = 3  # number of points used as input for each forecast\n",
    "    #\n",
    "    train_df = data_dict[serie_sp]['train']\n",
    "    test_df = data_dict[serie_sp]['test']\n",
    "    # filtra a serie, remove os valores nan e remove o id da serie(presente no 1 valor)\n",
    "    train_serie = train_df[train_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "    test_serie = test_df[test_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "    test_serie = test_serie[:fh] # forecast only fh steps\n",
    "    train_serie = np.asarray(train_serie, dtype=np.float32)\n",
    "    test_serie = np.asarray(test_serie, dtype=np.float32)\n",
    "    ts = np.concatenate([train_serie, test_serie], dtype=np.float32)\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # remove seasonality\n",
    "    seasonality_in = deseasonalize(ts, freq)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "    # detrending\n",
    "    a, b = detrend(ts)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] - ((a * i) + b)\n",
    "    # scaler = MinMaxScaler()\n",
    "    # scaler.fit(ts[:-fh].reshape(-1, 1))\n",
    "    # ts = scaler.transform(ts.reshape(-1, 1)).flatten()\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    # y_hat_test_MLP = mlp_bench(x_train.copy(), y_train.copy(), x_test.copy(), fh)\n",
    "\n",
    "    # ts = scaler.inverse_transform(ts.reshape(-1, 1)).flatten()\n",
    "    # y_hat_test_MLP = scaler.inverse_transform(y_hat_test_MLP.reshape(-1, 1)).flatten()\n",
    "    # y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    y_hat_test_MLP = mlp_bench(x_train, y_train, x_test, fh)\n",
    "    for i in range(0, 29):\n",
    "        y_hat_test_MLP = np.vstack((y_hat_test_MLP, mlp_bench(x_train, y_train, x_test, fh)))\n",
    "    y_hat_test_MLP = np.median(y_hat_test_MLP, axis=0)\n",
    "\n",
    "    # add trend\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] + ((a * i) + b)\n",
    "\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test_MLP[i] = y_hat_test_MLP[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "\n",
    "    # add seasonality\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    for i in range(len(ts), len(ts) + fh):\n",
    "        y_hat_test_MLP[i - len(ts)] = y_hat_test_MLP[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    # check if negative or extreme\n",
    "    for i in range(len(y_hat_test_MLP)):\n",
    "        if y_hat_test_MLP[i] < 0:\n",
    "            y_hat_test_MLP[i] = 0\n",
    "            \n",
    "        if y_hat_test_MLP[i] > (1000 * max(ts)):\n",
    "            y_hat_test_MLP[i] = max(ts)   \n",
    "\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "\n",
    "    \n",
    "    # Create Train examples for MLP and RNN\n",
    "   \n",
    "    # train_x, train_y, test_x, test_y = split_into_train_test(train_serie, test_serie, in_size)\n",
    "    \n",
    "    #Naive Forecast (Benchmark)\n",
    "    \n",
    "    # pred_y =  naive_predict(train_serie, test_serie, fh)\n",
    "    # ERRORS['naive_1'][serie_sp]['sMAPE'].append(smape(test_serie, pred_y))\n",
    "    # ERRORS['naive_1'][serie_sp]['MASE'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "    \n",
    "    # MLP (Benchmark)\n",
    "    #\n",
    "    ERRORS['mlp'][serie_sp]['sMAPE'].append(smape(y_test, y_hat_test_MLP))\n",
    "    ERRORS['mlp'][serie_sp]['MASE'].append(mase(ts[:-fh], y_test, y_hat_test_MLP, freq))  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/359 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 359/359 [08:23<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "ERRORS = {\n",
    "    'naive_1':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS},\n",
    "    'mlp':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS}\n",
    "    }\n",
    "\n",
    "for serie_index in tqdm(range(len(df_info))):\n",
    "    serie_info = df_info.iloc[serie_index]\n",
    "    serie_id = serie_info.M4id\n",
    "    serie_sp = serie_info.SP\n",
    "    fh = forecast_horizon[serie_sp]\n",
    "    freq = frequency[serie_sp]\n",
    "    in_size = 3  # number of points used as input for each forecast\n",
    "    #\n",
    "    train_df = data_dict[serie_sp]['train']\n",
    "    test_df = data_dict[serie_sp]['test']\n",
    "    # filtra a serie, remove os valores nan e remove o id da serie(presente no 1 valor)\n",
    "    train_serie = train_df[train_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "    test_serie = test_df[test_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "    test_serie = test_serie[:fh] # forecast only fh steps\n",
    "    train_serie = np.asarray(train_serie, dtype=np.float32)\n",
    "    test_serie = np.asarray(test_serie, dtype=np.float32)\n",
    "    ts = np.concatenate([train_serie, test_serie], dtype=np.float32)\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # remove seasonality\n",
    "    seasonality_in = deseasonalize(ts, freq)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "    # detrending\n",
    "    a, b = detrend(ts)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] - ((a * i) + b)\n",
    "    # scaler = MinMaxScaler()\n",
    "    # scaler.fit(ts[:-fh].reshape(-1, 1))\n",
    "    # ts = scaler.transform(ts.reshape(-1, 1)).flatten()\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    # y_hat_test_MLP = mlp_bench(x_train.copy(), y_train.copy(), x_test.copy(), fh)\n",
    "\n",
    "    # ts = scaler.inverse_transform(ts.reshape(-1, 1)).flatten()\n",
    "    # y_hat_test_MLP = scaler.inverse_transform(y_hat_test_MLP.reshape(-1, 1)).flatten()\n",
    "    # y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    y_hat_test_MLP = mlp_bench(x_train, y_train, x_test, fh)\n",
    "    for i in range(0, 29):\n",
    "        y_hat_test_MLP = np.vstack((y_hat_test_MLP, mlp_bench(x_train, y_train, x_test, fh)))\n",
    "    y_hat_test_MLP = np.median(y_hat_test_MLP, axis=0)\n",
    "\n",
    "    # add trend\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] + ((a * i) + b)\n",
    "\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test_MLP[i] = y_hat_test_MLP[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "\n",
    "    # add seasonality\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    for i in range(len(ts), len(ts) + fh):\n",
    "        y_hat_test_MLP[i - len(ts)] = y_hat_test_MLP[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    # check if negative or extreme\n",
    "    for i in range(len(y_hat_test_MLP)):\n",
    "        if y_hat_test_MLP[i] < 0:\n",
    "            y_hat_test_MLP[i] = 0\n",
    "            \n",
    "        if y_hat_test_MLP[i] > (1000 * max(ts)):\n",
    "            y_hat_test_MLP[i] = max(ts)   \n",
    "\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "\n",
    "    \n",
    "    # Create Train examples for MLP and RNN\n",
    "   \n",
    "    # train_x, train_y, test_x, test_y = split_into_train_test(train_serie, test_serie, in_size)\n",
    "    \n",
    "    #Naive Forecast (Benchmark)\n",
    "    \n",
    "    # pred_y =  naive_predict(train_serie, test_serie, fh)\n",
    "    # ERRORS['naive_1'][serie_sp]['sMAPE'].append(smape(test_serie, pred_y))\n",
    "    # ERRORS['naive_1'][serie_sp]['MASE'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "    \n",
    "    # MLP (Benchmark)\n",
    "    #\n",
    "    ERRORS['mlp'][serie_sp]['sMAPE'].append(smape(y_test, y_hat_test_MLP))\n",
    "    ERRORS['mlp'][serie_sp]['MASE'].append(mase(ts[:-fh], y_test, y_hat_test_MLP, freq))  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------FINAL RESULTS---------\n",
      "Model : naive_1\n",
      "  Weekly: \n",
      "    sMAPE: nan\n",
      "    MASE: nan\n",
      "Model : mlp\n",
      "  Weekly: \n",
      "    sMAPE: 19.556\n",
      "    MASE: 13.512\n"
     ]
    }
   ],
   "source": [
    "print(\"---------FINAL RESULTS---------\")\n",
    "for model, err in ERRORS.items():\n",
    "    print(f'Model : {model}')\n",
    "    for sp, sp_err in err.items():\n",
    "        print(f'  {sp}: ')\n",
    "        print(f'    sMAPE: {np.mean(sp_err[\"sMAPE\"])*100:.3f}')\n",
    "        print(f'    MASE: {np.mean(sp_err[\"MASE\"]):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Loop (Multiple Serie Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERRORS = {\n",
    "    'naive_1':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS},\n",
    "    'mlp':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERRORS = {\n",
    "    'naive_1':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS},\n",
    "    'mlp':{p:{'sMAPE':[],'MASE':[]} for p in USED_PERIODS}\n",
    "    }\n",
    "\n",
    "for serie_index in tqdm(range(len(df_info))):\n",
    "    serie_info = df_info.iloc[serie_index]\n",
    "    serie_id = serie_info.M4id\n",
    "    serie_sp = serie_info.SP\n",
    "    fh = forecast_horizon[serie_sp]\n",
    "    freq = frequency[serie_sp]\n",
    "    in_size = 3  # number of points used as input for each forecast\n",
    "    #\n",
    "    train_df = data_dict[serie_sp]['train']\n",
    "    test_df = data_dict[serie_sp]['test']\n",
    "    # filtra a serie, remove os valores nan e remove o id da serie(presente no 1 valor)\n",
    "    train_serie = train_df[train_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "    test_serie = test_df[test_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "    test_serie = test_serie[:fh] # forecast only fh steps\n",
    "    train_serie = np.asarray(train_serie, dtype=np.float32)\n",
    "    test_serie = np.asarray(test_serie, dtype=np.float32)\n",
    "    ts = np.concatenate([train_serie, test_serie], dtype=np.float32)\n",
    "    #\n",
    "    #\n",
    "    #\n",
    "    # remove seasonality\n",
    "    seasonality_in = deseasonalize(ts, freq)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * 100 / seasonality_in[i % freq]\n",
    "\n",
    "    # detrending\n",
    "    a, b = detrend(ts)\n",
    "\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] - ((a * i) + b)\n",
    "    # scaler = MinMaxScaler()\n",
    "    # scaler.fit(ts[:-fh].reshape(-1, 1))\n",
    "    # ts = scaler.transform(ts.reshape(-1, 1)).flatten()\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    # y_hat_test_MLP = mlp_bench(x_train.copy(), y_train.copy(), x_test.copy(), fh)\n",
    "\n",
    "    # ts = scaler.inverse_transform(ts.reshape(-1, 1)).flatten()\n",
    "    # y_hat_test_MLP = scaler.inverse_transform(y_hat_test_MLP.reshape(-1, 1)).flatten()\n",
    "    # y_test = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    # MLP benchmark - Produce forecasts\n",
    "    y_hat_test_MLP = mlp_bench(x_train, y_train, x_test, fh)\n",
    "    for i in range(0, 29):\n",
    "        y_hat_test_MLP = np.vstack((y_hat_test_MLP, mlp_bench(x_train, y_train, x_test, fh)))\n",
    "    y_hat_test_MLP = np.median(y_hat_test_MLP, axis=0)\n",
    "\n",
    "    # add trend\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] + ((a * i) + b)\n",
    "\n",
    "    for i in range(0, fh):\n",
    "        y_hat_test_MLP[i] = y_hat_test_MLP[i] + ((a * (len(ts) + i + 1)) + b)\n",
    "\n",
    "    # add seasonality\n",
    "    for i in range(0, len(ts)):\n",
    "        ts[i] = ts[i] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    for i in range(len(ts), len(ts) + fh):\n",
    "        y_hat_test_MLP[i - len(ts)] = y_hat_test_MLP[i - len(ts)] * seasonality_in[i % freq] / 100\n",
    "\n",
    "    # check if negative or extreme\n",
    "    for i in range(len(y_hat_test_MLP)):\n",
    "        if y_hat_test_MLP[i] < 0:\n",
    "            y_hat_test_MLP[i] = 0\n",
    "            \n",
    "        if y_hat_test_MLP[i] > (1000 * max(ts)):\n",
    "            y_hat_test_MLP[i] = max(ts)   \n",
    "\n",
    "    x_train, y_train, x_test, y_test = split_into_train_test(ts, in_size, fh)\n",
    "\n",
    "    \n",
    "    # Create Train examples for MLP and RNN\n",
    "   \n",
    "    # train_x, train_y, test_x, test_y = split_into_train_test(train_serie, test_serie, in_size)\n",
    "    \n",
    "    #Naive Forecast (Benchmark)\n",
    "    \n",
    "    # pred_y =  naive_predict(train_serie, test_serie, fh)\n",
    "    # ERRORS['naive_1'][serie_sp]['sMAPE'].append(smape(test_serie, pred_y))\n",
    "    # ERRORS['naive_1'][serie_sp]['MASE'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "    \n",
    "    # MLP (Benchmark)\n",
    "    #\n",
    "    ERRORS['mlp'][serie_sp]['sMAPE'].append(smape(y_test, y_hat_test_MLP))\n",
    "    ERRORS['mlp'][serie_sp]['MASE'].append(mase(ts[:-fh], y_test, y_hat_test_MLP, freq))  \n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
