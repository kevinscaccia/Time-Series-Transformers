{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M4 Dataset Transfer Learning Training\n",
    "> Code that train the Vanilla Transformer on all train series from M4 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    => Weekly has 359 series\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mget_batches(n_series\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m dataset \u001b[38;5;241m=\u001b[39m generator\u001b[38;5;241m.\u001b[39mget_batches(n_series\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 9\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_series\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/utils/m4.py:374\u001b[0m, in \u001b[0;36mMultiSerieGenerator.get_batches\u001b[0;34m(self, n_series, random, seed)\u001b[0m\n\u001b[1;32m    372\u001b[0m     all_tgt_y\u001b[38;5;241m.\u001b[39mappend(tgt_y)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# stack\u001b[39;00m\n\u001b[0;32m--> 374\u001b[0m all_enc_x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_enc_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m all_dec_x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(all_dec_x)\n\u001b[1;32m    376\u001b[0m all_tgt_y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack(all_tgt_y)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "from utils.m4 import MultiSerieGenerator\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(777)\n",
    "generator =  MultiSerieGenerator(['Weekly'], input_len=1000, forecast_horizon=10,device='cuda')\n",
    "dataset = generator.get_batches(n_series=2, random=True)\n",
    "dataset = generator.get_batches(n_series=2, random=True)\n",
    "dataset = generator.get_batches(n_series=2, random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.m4 import load_m4_data\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils.ml import make_batches, TransformerDataset\n",
    "import torch\n",
    "\n",
    "class MultiSerieGenerator():\n",
    "    def __init__(self, freq, input_len, forecast_horizon):\n",
    "        self.input_len = input_len\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.device = 'cuda'\n",
    "        print('Loading M4 Data...')\n",
    "        self.data_dict, self.df_info = load_m4_data(freq)\n",
    "        print('Loaded:')\n",
    "        for SP in freq:print(f\"    => {SP} has {self.data_dict[SP]['num']} series\")\n",
    "    \n",
    "    def get_batches(self, n_series=None, random=False, seed=None, verbose=False):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        df_info, data_dict = self.df_info, self.data_dict\n",
    "        if n_series is None:\n",
    "            n_series = len(df_info)\n",
    "        if random:\n",
    "            idx = np.random.randint(low=0, high=len(df_info), size=n_series)\n",
    "        else:\n",
    "            idx = range(n_series)\n",
    "        if verbose: print(f'Generating {len(idx)} series..')\n",
    "        # faz o scalind individual das series completas\n",
    "        scaler = MinMaxScaler((-1, 1))\n",
    "        all_enc_x, all_dec_x, all_tgt_y  = [], [], []\n",
    "        for serie_index in idx:\n",
    "            serie_info = df_info.iloc[serie_index]\n",
    "            serie_id = serie_info.M4id\n",
    "            print(serie_id, end=', ')\n",
    "            serie_sp = serie_info.SP\n",
    "            fh = data_dict[serie_sp]['fh']\n",
    "            freq = data_dict[serie_sp]['freq']\n",
    "            train_df = data_dict[serie_sp]['train']\n",
    "            test_df = data_dict[serie_sp]['test']\n",
    "            # the V1 column is the name of the serie\n",
    "            train_serie = train_df[train_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "            test_serie = test_df[test_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "            test_serie = test_serie[:fh] # forecast only fh steps\n",
    "            train_serie = np.asarray(train_serie, dtype=np.float32)\n",
    "            test_serie = np.asarray(test_serie, dtype=np.float32)\n",
    "            #\n",
    "            train_serie = scaler.fit_transform(train_serie.reshape(-1, 1)).reshape(-1)\n",
    "            test_serie = scaler.transform(test_serie.reshape(-1, 1)).reshape(-1)\n",
    "            #\n",
    "            enc_x, dec_x, tgt_y = make_batches(train_serie, self.input_len, self.forecast_horizon)\n",
    "            # enc_x, dec_x, tgt_y = enc_x.to(self.device), dec_x.to(self.device), tgt_y.to(self.device)\n",
    "            all_enc_x.append(enc_x)\n",
    "            all_dec_x.append(dec_x)\n",
    "            all_tgt_y.append(tgt_y)\n",
    "        # stack\n",
    "        all_enc_x = torch.vstack(all_enc_x)\n",
    "        all_dec_x = torch.vstack(all_dec_x)\n",
    "        all_tgt_y = torch.vstack(all_tgt_y)\n",
    "        # shuffle\n",
    "        all_enc_x = all_enc_x.to(self.device)\n",
    "        all_dec_x = all_dec_x.to(self.device)\n",
    "        all_tgt_y = all_tgt_y.to(self.device)\n",
    "        print(f'Generated {len(all_enc_x)} batches from {len(idx)} series-> shape {all_enc_x.shape}')\n",
    "\n",
    "        return  TransformerDataset(all_enc_x, all_dec_x, all_tgt_y)\n",
    "\n",
    "np.random.seed(777)\n",
    "generator =  MultiSerieGenerator(['Weekly'], input_len=10, forecast_horizon=10)\n",
    "dataset = generator.get_batches(n_series=5, random=True, verbose=False)\n",
    "dataset = generator.get_batches(n_series=5, random=True, verbose=False)\n",
    "dataset = generator.get_batches(n_series=5, random=True, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading M4 Data...\n",
      "Loaded:\n",
      "    => Weekly has 359 series\n",
      "W104, W304, W60, W88, W72, Generated 5418 batches from 5 series-> shape torch.Size([5418, 10, 1])\n",
      "W158, W128, W117, W303, W281, Generated 5585 batches from 5 series-> shape torch.Size([5585, 10, 1])\n",
      "W296, W143, W33, W348, W322, Generated 2792 batches from 5 series-> shape torch.Size([2792, 10, 1])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile experiment.py\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from utils.timeserie import split_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error\n",
    "import mlflow\n",
    "#\n",
    "from utils.ml import SimpleDataset, TransformerDataset\n",
    "from utils.plot import plot_train_history, plot_predictions\n",
    "from utils.m4 import deseasonalize, detrend, smape\n",
    "# Transformers\n",
    "from utils.ml import make_batches\n",
    "from utils.plot import generate_square_subsequent_mask\n",
    "#\n",
    "class Experiment():\n",
    "\n",
    "    def __init__(self, config: dict):\n",
    "        # Set experiment config\n",
    "        expected_vars = ['model','input_len','feature_dim','frequency',\n",
    "                         'device','scaler','verbose','freq','decompose',\n",
    "                         'forecast_horizon',]\n",
    "        for v in expected_vars:\n",
    "            assert v in config.keys(), f'Key \"{v}\" is missing on params dict'\n",
    "        for k, v in config.items():\n",
    "            vars(self)[k] = v\n",
    "        self.config = config\n",
    "        #\n",
    "        # Pre-configuration (to produce same result in inference/predict)\n",
    "        #\n",
    "        np.random.seed(7); torch.manual_seed(7)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(7)\n",
    "        #\n",
    "        #\n",
    "        #\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.validation_dataset = None\n",
    "        self.train_dataset = None\n",
    "\n",
    "    def split_chunks(self, linear_serie, expand_dim=True):\n",
    "        x, y = split_sequence(linear_serie, self.input_len)\n",
    "        x, y = torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "        if expand_dim:\n",
    "            x, y = x.unsqueeze(-1), y.unsqueeze(-1)\n",
    "        return x, y\n",
    "    \n",
    "    def ts_transform(self, ts, train=False):\n",
    "        # de-sazonality\n",
    "        if train: # calculate\n",
    "            if self.verbose: print('Decomposition FIT')\n",
    "            self.seasonality_in = deseasonalize(ts, self.freq)\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * 100 / self.seasonality_in[i % self.freq]\n",
    "\n",
    "        # de-trending\n",
    "        if train: # calculate\n",
    "            self.a, self.b = detrend(ts)   \n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] - ((self.a * i) + self.b)\n",
    "        \n",
    "        return ts\n",
    "\n",
    "    def ts_inverse_transform(self, ts):\n",
    "        # add trend\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] + ((self.a * i) + self.b)\n",
    "        # add seasonality\n",
    "        for i in range(0, len(ts)):\n",
    "            ts[i] = ts[i] * self.seasonality_in[i % self.freq] / 100\n",
    "        \n",
    "        return ts\n",
    "    \n",
    "    def set_dataset(self, linear_serie, train=False, validation=False):\n",
    "        linear_serie = linear_serie.copy()\n",
    "        if self.decompose:\n",
    "            linear_serie = self.ts_transform(linear_serie, train)\n",
    "\n",
    "        if self.scaler is not None:\n",
    "            if train: # FIT Scaler\n",
    "                linear_serie = self.scaler.fit_transform(linear_serie.reshape(-1,1)).reshape(-1)\n",
    "                if self.verbose: print('Scaler FIT')\n",
    "                \n",
    "            if validation:\n",
    "                linear_serie = self.scaler.transform(linear_serie.reshape(-1,1)).reshape(-1)\n",
    "        \n",
    "        # transformer specific input shape\n",
    "        if self.model.is_transformer():\n",
    "            enc_x, dec_x, tgt_y = make_batches(linear_serie, \n",
    "                                               self.input_len, self.forecast_horizon)\n",
    "            enc_x, dec_x, tgt_y = enc_x.to(self.device), dec_x.to(self.device), tgt_y.to(self.device)\n",
    "            data = TransformerDataset(enc_x, dec_x, tgt_y)\n",
    "        else: # naive, cnn, mlp, lstm, \n",
    "            x, y = self.split_chunks(linear_serie)\n",
    "            x, y = x.to(self.device), y.to(self.device)\n",
    "            data = SimpleDataset(x, y)\n",
    "        # Save\n",
    "        if train:\n",
    "            self.train_dataset = data\n",
    "        if validation:\n",
    "            self.validation_dataset = data\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def train(self, train_conf):\n",
    "        train_conf['train_dataset'] = self.train_dataset\n",
    "        train_conf['validation_dataset'] = self.validation_dataset\n",
    "        self.model.fit(train_conf)\n",
    "    \n",
    "    def train_history(self, offset=0):\n",
    "        plot_train_history(self.model.train_loss_history, self.model.validation_loss_history, offset)\n",
    "    \n",
    "    def preprocess(self, ts): # from numpy to torch\n",
    "        ts = np.asarray(ts, dtype=np.float32).copy()\n",
    "        if self.decompose:\n",
    "            ts = self.ts_transform(ts, train=False)\n",
    "        if self.scaler is not None:\n",
    "            ts = self.scaler.transform(ts.reshape(-1,1)).reshape(-1)\n",
    "        ts = torch.tensor(ts, dtype=torch.float32)\\\n",
    "                                .view(1,-1, 1).to(self.device)\n",
    "        return ts\n",
    "\n",
    "    def posprocess(self, ts): # from torch to numpy\n",
    "        ts = ts.to('cpu').detach().numpy()\n",
    "        if self.scaler is not None:\n",
    "            ts = self.scaler.inverse_transform(ts.reshape(-1,1)).reshape(-1)\n",
    "        if self.decompose:\n",
    "            ts = self.ts_inverse_transform(ts)\n",
    "        # ts = ts.to(self.device)\n",
    "        return ts.flatten()\n",
    "\n",
    "    def predict(self, ts, forecast_horizon):\n",
    "        # assert(len(ts) == self.input_len) \n",
    "        ts = self.preprocess(ts) # TODO add ts copy \n",
    "        output = self.model.predict(ts, forecast_horizon)\n",
    "        # rescale and add trend, etc..\n",
    "        output = self.posprocess(output)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def print_metrics(self, real_y, pred_y):\n",
    "        mape = mean_absolute_percentage_error(real_y, pred_y)\n",
    "        err_smape = smape(real_y, pred_y)\n",
    "        mse = mean_squared_error(real_y, pred_y)\n",
    "        print(f'MSE: {mse:.4f} | MAPE: {mape:.4f} | sMAPE: {err_smape:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import argparse\n",
    "import numpy as np\n",
    "# ml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mlflow\n",
    "# local\n",
    "from models.benchmark import NaivePredictor\n",
    "from models.cnn import SimpleCNN\n",
    "from models.transformer import VanillaTransformer\n",
    "from utils.plot import plot_predictions\n",
    "from utils.ml import EarlyStopperPercent\n",
    "from experiment import Experiment\n",
    "from utils.m4 import smape, mase, M4DatasetGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading M4 Data...\n",
      "Loaded:\n",
      "    => Weekly has 359 series\n"
     ]
    }
   ],
   "source": [
    "model_name = ['vanilla_transformer','cnn'][0]\n",
    "run_sp = 'Weekly'\n",
    "\n",
    "assert(model_name in ['cnn','naive', 'vanilla_transformer'])\n",
    "assert run_sp in ['Hourly','Daily','Weekly','Monthly','Quarterly','Yearly']\n",
    "m4_data = M4DatasetGenerator([run_sp])\n",
    "\n",
    "def get_model(model_name, model_conf):\n",
    "    if model_name == 'cnn':\n",
    "        return SimpleCNN(model_conf['block_size'], model_conf['d_model'])\n",
    "    elif model_name == 'naive':\n",
    "        return NaivePredictor()\n",
    "    elif model_name == 'vanilla_transformer':\n",
    "        return  VanillaTransformer(model_conf)\n",
    "    \n",
    "if TRACK:\n",
    "    mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "    mlflow.set_experiment(f\"M4Benchmark {model_name}\")\n",
    "    mlflow.set_experiment_tag('model', model_name)\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 168833\n",
      "Full Pass     1:      smape_mean: 2.193      mase_mean: 2.033      smape_std: 0.388      mase_std: 1.214\n",
      "Full Pass     2:      smape_mean: 3.131      mase_mean: 2.657      smape_std: 0.044      mase_std: 1.238\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Inicializations\n",
    "#\n",
    "num_of_series = m4_data.data_dict[run_sp]['num']\n",
    "block_size = m4_data.data_dict[run_sp]['fh']\n",
    "fh = m4_data.data_dict[run_sp]['fh']\n",
    "#\n",
    "# Model Hiperparams\n",
    "#\n",
    "d_model = 64\n",
    "batch_size = 512 #512\n",
    "epochs = 512\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "decompose = False\n",
    "MAX_SERIES = 2\n",
    "FULL_PASSES = 50\n",
    "#\n",
    "# model_conf = {'block_size':block_size, 'd_model':d_model}\n",
    "if model_name == 'vanilla_transformer':\n",
    "    model_conf = {'block_size':block_size, 'd_model': d_model, 'num_heads': 2, 'num_layers': 2,'dim_feedforward':128,'device':'cuda'}\n",
    "    lr = 1e-4\n",
    "else:\n",
    "    lr = 1e-3\n",
    "model = get_model(model_name, model_conf)\n",
    "model_n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Params: {model_n_parameters}')\n",
    "# model\n",
    "\n",
    "for full_pass_i in range(FULL_PASSES):\n",
    "    metrics_table = {'serie_id':[],'smape':[],'mase':[],}\n",
    "    smape_list, mase_list = [], []\n",
    "    for train_serie, test_serie, serie_id, fh, freq, serie_sp in m4_data.generate(n_series=MAX_SERIES, random=True):\n",
    "        assert fh == block_size\n",
    "        exp_conf = {\n",
    "                # Model\n",
    "                'model': model,\n",
    "                'model_n_parameters': model_n_parameters, \n",
    "                'input_len':block_size,\n",
    "                'forecast_horizon':fh,\n",
    "                'feature_dim':1,\n",
    "                # Data\n",
    "                'frequency':serie_sp.lower(),\n",
    "                'scaler':scaler,\n",
    "                'decompose': decompose, #detrend and de-sazonalize\n",
    "                'freq':freq,\n",
    "                # Others\n",
    "                'device':'cuda',\n",
    "                'verbose':False,\n",
    "        }\n",
    "        train_conf = {\n",
    "            'epochs':epochs,\n",
    "            'lr':lr, \n",
    "            'batch_size':batch_size,\n",
    "            'verbose':False, # stop training if loss dont decrease 0.5% 5 consecutive steps\n",
    "            'early_stop':EarlyStopperPercent(patience=5, min_percent=0.005, verbose=False),\n",
    "        }\n",
    "        exp = Experiment(exp_conf)\n",
    "        exp.set_dataset(linear_serie=train_serie, train=True)\n",
    "        # exp.set_dataset(linear_serie=test_serie)\n",
    "        exp.train(train_conf)\n",
    "        # test\n",
    "        last_train_values = train_serie[-block_size:]\n",
    "        pred_y = exp.predict(last_train_values, fh)\n",
    "        \n",
    "        # check if negative or extreme (M4)\n",
    "        pred_y[pred_y < 0] = 0\n",
    "        pred_y[pred_y > (1000 * np.max(train_serie))] = np.max(train_serie)\n",
    "\n",
    "        # Metrics\n",
    "        metrics_table['serie_id'].append(serie_id)\n",
    "        metrics_table['smape'].append(smape(test_serie, pred_y)*100)\n",
    "        metrics_table['mase'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "        # print(f'Serie {serie_id}-{serie_sp} Finished -> smape: {smape(test_serie, pred_y)*100} | mase:{mase(train_serie, test_serie, pred_y, freq)}')\n",
    "        # plot_predictions(train_serie, test_serie, pred_y)\n",
    "        \n",
    "    #\n",
    "    metrics_dict = {\n",
    "        'smape_mean': np.round(np.mean(metrics_table['smape'], dtype=float), 3), \n",
    "        'mase_mean':  np.round(np.mean(metrics_table['mase'], dtype=float), 3),\n",
    "        #\n",
    "        'smape_std':  np.round(np.std(metrics_table['smape'], dtype=float), 3),\n",
    "        'mase_std':   np.round(np.std(metrics_table['mase'], dtype=float), 3),\n",
    "    }\n",
    "    if TRACK:\n",
    "        mlflow.log_metrics(metrics_dict)\n",
    "        mlflow.log_table(metrics_table, artifact_file='metrics_table')\n",
    "\n",
    "    print(f'Full Pass {1+full_pass_i:5}:', end='')\n",
    "    for k, v in metrics_dict.items(): print(f'      {k}: {v}', end='')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
