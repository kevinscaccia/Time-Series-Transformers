{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M4 Dataset Benchmark Code\n",
    "> Generic code to experiment and produce the final benchmark.py codes \\\n",
    "\n",
    "\n",
    "The minimum numbers of observations in the training\n",
    "- 13 for yearly\n",
    "- 16 for quarterly\n",
    "- 42 for monthly\n",
    "- 80 for weekly\n",
    "- 93 for daily\n",
    "- 700 for hourly series.\n",
    "\n",
    "\n",
    "\n",
    "Cada batch é composto de sequencias de diferentes séries, com no máximo block_size steps. \\\n",
    "Caso a serie possuir menos de block_size instantes, realiza-se o padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import numpy as np\n",
    "# ml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mlflow\n",
    "# local\n",
    "from models.benchmark import NaivePredictor\n",
    "from models.cnn import SimpleCNN\n",
    "from models.transformer import VanillaTransformer, DecoderOnlyTransformer\n",
    "from utils.plot import plot_predictions\n",
    "from utils.ml import EarlyStopperPercent\n",
    "from experiment import Experiment\n",
    "from utils.m4 import smape, mase\n",
    "from utils.m4 import load_m4_data\n",
    "import pandas as pd, numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRACK = False\n",
    "PAD = -20\n",
    "model_name = ['cnn','naive', 'vanilla_transformer','decoder_transformer'][3]\n",
    "run_sp = 'Weekly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name, model_conf):\n",
    "    if model_name == 'cnn':\n",
    "        return SimpleCNN(model_conf['block_size'], model_conf['d_model'])\n",
    "    elif model_name == 'naive':\n",
    "        return NaivePredictor()\n",
    "    elif model_name == 'vanilla_transformer':\n",
    "        return  VanillaTransformer(model_conf)\n",
    "    elif model_name == 'decoder_transformer':\n",
    "        return  DecoderOnlyTransformer(model_conf)\n",
    "    else: \n",
    "        raise Exception('Undefined Model')\n",
    "    \n",
    "    \n",
    "if TRACK:\n",
    "    mlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n",
    "    mlflow.set_experiment(f\"M4Benchmark {model_name}\")\n",
    "    mlflow.set_experiment_tag('model', model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_x_y(data, block_size):\n",
    "    if len(data) > block_size:\n",
    "        idx_start = torch.arange(0, len(data)-block_size)\n",
    "        # idx_start = torch.randint(len(data)-block_size, (batch_size,))\n",
    "        x = torch.stack([data[i:i+block_size] for i in idx_start])\n",
    "        y = torch.stack([data[i+1:i+block_size+1] for i in idx_start])\n",
    "        x_pad = (x == PAD)\n",
    "        return x, y, x_pad\n",
    "    else: # need to pad\n",
    "        x = np.pad(data, (0, block_size-len(data)), constant_values=PAD).reshape(1, -1)# batch\n",
    "        # y = data[1:].reshape(1, -1)# batch\n",
    "        y = np.pad(data[1:], (0, block_size-len(data[1:])), constant_values=PAD).reshape(1, -1)# batch\n",
    "        x_pad = (x == PAD)\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), torch.tensor(x_pad, dtype=torch.float32)\n",
    "#\n",
    "# Test\n",
    "#\n",
    "def test_get_x_y():\n",
    "    data = torch.arange(1,200)\n",
    "    x, y, x_pad = get_x_y(data, block_size=10)\n",
    "    assert(x[0, 1] == y[0, 0]) # \n",
    "\n",
    "    x, y, x_pad = get_x_y(data, block_size=1000)\n",
    "    assert (x.shape == (1, 1000))\n",
    "    assert (x[0, len(data)+1] == PAD)\n",
    "\n",
    "test_get_x_y() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiSerieGenerator():\n",
    "    def __init__(self, freq, device, verbose=False):\n",
    "        self.device = device\n",
    "        self.verbose = verbose\n",
    "        if verbose: print('Loading M4 Data...')\n",
    "        self.data_dict, self.df_info = load_m4_data(freq)\n",
    "        if verbose: print('Loaded:')\n",
    "        for SP in freq:print(f\"    => {SP} has {self.data_dict[SP]['num']} series\")\n",
    "    \n",
    "    def get_batches(self, block_size, n_series=None, random=False, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        df_info, data_dict = self.df_info, self.data_dict\n",
    "        if n_series is None:\n",
    "            n_series = len(df_info)\n",
    "        else:\n",
    "            n_series = min(n_series, len(df_info))\n",
    "        #\n",
    "        if random:\n",
    "            idx = np.random.randint(low=0, high=len(df_info), size=n_series)\n",
    "        else:\n",
    "            idx = range(n_series)\n",
    "        if self.verbose: print(f'Generating {len(idx)} series..')\n",
    "        # faz o scalind individual das series completas\n",
    "        scaler = MinMaxScaler((-1, 1))\n",
    "        batch_x, batch_y, batch_masks = [], [], []\n",
    "        for serie_index in idx:\n",
    "            \n",
    "            serie_info = df_info.iloc[serie_index]\n",
    "            serie_id = serie_info.M4id\n",
    "            if self.verbose: print(serie_id, end=', ')\n",
    "            serie_sp = serie_info.SP\n",
    "            train_df = data_dict[serie_sp]['train']\n",
    "            \n",
    "            # the V1 column is the name of the serie\n",
    "            train_serie = train_df[train_df.V1 == serie_id].dropna(axis=1).values.reshape(-1)[1:]\n",
    "            #\n",
    "            train_serie = scaler.fit_transform(np.asarray(train_serie, dtype=np.float32).reshape(-1, 1)).reshape(-1)\n",
    "            train_serie = torch.tensor(train_serie, dtype=torch.float32)\n",
    "            x, y, x_pad = get_x_y(train_serie, block_size=block_size)\n",
    "            batch_x.append(x), batch_y.append(y), batch_masks.append(x_pad)\n",
    "        #\n",
    "        batch_x = torch.vstack(batch_x).unsqueeze(-1).to(self.device)\n",
    "        batch_y = torch.vstack(batch_y).unsqueeze(-1).to(self.device)\n",
    "        batch_masks = torch.vstack(batch_masks).to(self.device)\n",
    "\n",
    "        return batch_x, batch_y, batch_masks#TransformerDataset(all_enc_x, all_dec_x, all_tgt_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.transformer import DecoderOnlyTransformer\n",
    "from utils.ml import DecoderDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of weights: 67297\n",
      "Loading M4 Data...\n",
      "Loaded:\n",
      "    => Weekly has 359 series\n",
      "Generating 359 series..\n",
      "W1, W2, W3, W4, W5, W6, W7, W8, W9, W10, W11, W12, W13, W14, W15, W16, W17, W18, W19, W20, W21, W22, W23, W24, W25, W26, W27, W28, W29, W30, W31, W32, W33, W34, W35, W36, W37, W38, W39, W40, W41, W42, W43, W44, W45, W46, W47, W48, W49, W50, W51, W52, W53, W54, W55, W56, W57, W58, W59, W60, W61, W62, W63, W64, W65, W66, W67, W68, W69, W70, W71, W72, W73, W74, W75, W76, W77, W78, W79, W80, W81, W82, W83, W84, W85, W86, W87, W88, W89, W90, W91, W92, W93, W94, W95, W96, W97, W98, W99, W100, W101, W102, W103, W104, W105, W106, W107, W108, W109, W110, W111, W112, W113, W114, W115, W116, W117, W118, W119, W120, W121, W122, W123, W124, W125, W126, W127, W128, W129, W130, W131, W132, W133, W134, W135, W136, W137, W138, W139, W140, W141, W142, W143, W144, W145, W146, W147, W148, W149, W150, W151, W152, W153, W154, W155, W156, W157, W158, W159, W160, W161, W162, W163, W164, W165, W166, W167, W168, W169, W170, W171, W172, W173, W174, W175, W176, W177, W178, W179, W180, W181, W182, W183, W184, W185, W186, W187, W188, W189, W190, W191, W192, W193, W194, W195, W196, W197, W198, W199, W200, W201, W202, W203, W204, W205, W206, W207, W208, W209, W210, W211, W212, W213, W214, W215, W216, W217, W218, W219, W220, W221, W222, W223, W224, W225, W226, W227, W228, W229, W230, W231, W232, W233, W234, W235, W236, W237, W238, W239, W240, W241, W242, W243, W244, W245, W246, W247, W248, W249, W250, W251, W252, W253, W254, W255, W256, W257, W258, W259, W260, W261, W262, W263, W264, W265, W266, W267, W268, W269, W270, W271, W272, W273, W274, W275, W276, W277, W278, W279, W280, W281, W282, W283, W284, W285, W286, W287, W288, W289, W290, W291, W292, W293, W294, W295, W296, W297, W298, W299, W300, W301, W302, W303, W304, W305, W306, W307, W308, W309, W310, W311, W312, W313, W314, W315, W316, W317, W318, W319, W320, W321, W322, W323, W324, W325, W326, W327, W328, W329, W330, W331, W332, W333, W334, W335, W336, W337, W338, W339, W340, W341, W342, W343, W344, W345, W346, W347, W348, W349, W350, W351, W352, W353, W354, W355, W356, W357, W358, W359, "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Inicializations\n",
    "#\n",
    "block_size = 512\n",
    "n_series = 500\n",
    "#\n",
    "# Model Hiperparams\n",
    "#\n",
    "model = DecoderOnlyTransformer({\n",
    "    'd_model': 32, \n",
    "    'num_heads': 4, \n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward':128,\n",
    "    'block_size':block_size,\n",
    "    'device':'cuda',\n",
    "    'pad_token':PAD\n",
    "}).to('cuda')\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Num of weights:',pytorch_total_params)\n",
    "train_dataset = DecoderDataset(*MultiSerieGenerator(['Weekly'], device='cuda',verbose=True).get_batches(block_size, n_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.load('decoder_only_weekly_1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting train. 272 batches 216888/800\n",
      "Epoch 1/100 [497.712secs] -> Train loss: 0.04742\n",
      "Epoch 2/100 [501.675secs] -> Train loss: 0.01139\n",
      "Epoch 3/100 [506.052secs] -> Train loss: 0.00809\n",
      "Epoch 4/100 [506.472secs] -> Train loss: 0.00696\n",
      "Epoch 5/100 [506.294secs] -> Train loss: 0.00636\n",
      "Epoch 6/100 [505.678secs] -> Train loss: 0.00599\n",
      "Epoch 7/100 [504.128secs] -> Train loss: 0.00576\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      3\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-3\u001b[39m\n\u001b[1;32m      4\u001b[0m train_conf \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m:epochs,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m:lr, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m:train_dataset\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_conf\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/models/transformer.py:279\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.fit\u001b[0;34m(self, conf)\u001b[0m\n\u001b[1;32m    277\u001b[0m enc_x, tgt_y, mask_x  \u001b[38;5;241m=\u001b[39m enc_x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), tgt_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), mask_x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    278\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# current batch zero-out the loss\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m pred_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menc_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_x\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# mask x is very very important!!!\u001b[39;00m\n\u001b[1;32m    280\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred_y, tgt_y) \u001b[38;5;66;03m# loss with padding\u001b[39;00m\n\u001b[1;32m    281\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mwhere((tgt_y \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token), torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m))\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;66;03m# mask pading in the loss!\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Time-Series-Transformers/models/transformer.py:222\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.forward\u001b[0;34m(self, src, mask, pad_mask)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src: torch\u001b[38;5;241m.\u001b[39mTensor, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pad_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 222\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_embedding(src) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_emb(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;66;03m# (B, T) --> (B, T, Emb) \u001b[39;00m\n\u001b[1;32m    223\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(src, mask\u001b[38;5;241m=\u001b[39mmask, src_key_padding_mask\u001b[38;5;241m=\u001b[39mpad_mask)\n\u001b[1;32m    224\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer(pred)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 800\n",
    "epochs = 100\n",
    "lr = 1e-3\n",
    "train_conf = {\n",
    "    'epochs':epochs,\n",
    "    'lr':lr, \n",
    "    'batch_size':batch_size,\n",
    "    'verbose':True, # stop training if loss dont decrease 0.5% 5 consecutive steps\n",
    "    # 'early_stop':EarlyStopperPercent(patience=5, min_percent=0.005, verbose=False),\n",
    "    'train_dataset':train_dataset\n",
    "}\n",
    "model.fit(train_conf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading M4 Data...\n",
      "Loaded:\n",
      "    => Weekly has 359 series\n",
      "W1\n",
      "torch.Size([1, 13, 1]) (13,)\n"
     ]
    }
   ],
   "source": [
    "from utils.m4 import M4DatasetGenerator\n",
    "m4_data = M4DatasetGenerator(['Weekly'])\n",
    "scaler = MinMaxScaler((-1,1))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(self, src, forecast_horizon):\n",
    "    self.eval()\n",
    "    src = src[:, -self.block_size:, :].clone()\n",
    "    h_len = src.shape[1]\n",
    "    \n",
    "    for i in range(forecast_horizon):\n",
    "        x = src[:, -self.block_size:, :]\n",
    "        y = self(x)[:, -1:, :]\n",
    "        src = torch.concat((src, y), dim=1)\n",
    "    return src[:, h_len:, :]\n",
    "\n",
    "for train_serie, test_serie, serie_id, fh, eq, serie_sp in m4_data.generate(n_series=1, random=False):\n",
    "    print(serie_id)\n",
    "    scaler.fit(train_serie.reshape(-1, 1))    \n",
    "    x = scaler.transform(train_serie.reshape(-1, 1)).reshape(1, -1, 1)\n",
    "    x = torch.tensor(x, dtype=torch.float32).to('cuda')\n",
    "    # predict(model, x, len(test_serie))\n",
    "    pred_y = predict(model, x, len(test_serie))#.cpu().numpy()\n",
    "    print(pred_y.shape, test_serie.shape)\n",
    "    # # pred_y = model.predict(x, len(test_serie)).cpu().numpy()\n",
    "    # pred_y = scaler.inverse_transform(pred_y.reshape(-1,1)).reshape(-1)\n",
    "    # plot_predictions(train_serie, test_serie, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading M4 Data...\n",
      "Loaded:\n",
      "    => Weekly has 359 series\n",
      "Generating 2 series..\n",
      "W1, W1, "
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m block_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      2\u001b[0m m \u001b[38;5;241m=\u001b[39m MultiSerieGenerator([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeekly\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m X, M \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mget_batches(block_size\u001b[38;5;241m=\u001b[39mblock_size, n_series\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "block_size = 512\n",
    "m = MultiSerieGenerator(['Weekly'], device='cpu', verbose=True)\n",
    "X, M = m.get_batches(block_size=block_size, n_series=2)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'if' statement on line 24 (186095145.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 25\u001b[0;36m\u001b[0m\n\u001b[0;31m    mlflow.log_metrics(metrics_dict)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'if' statement on line 24\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "last_train_values = train_serie[-block_size:]\n",
    "pred_y = exp.predict(last_train_values, fh)\n",
    "\n",
    "# check if negative or extreme (M4)\n",
    "pred_y[pred_y < 0] = 0\n",
    "pred_y[pred_y > (1000 * np.max(train_serie))] = np.max(train_serie)\n",
    "\n",
    "# Metrics\n",
    "metrics_table['serie_id'].append(serie_id)\n",
    "metrics_table['smape'].append(smape(test_serie, pred_y)*100)\n",
    "metrics_table['mase'].append(mase(train_serie, test_serie, pred_y, freq))\n",
    "print(f'Serie {serie_id}-{serie_sp} Finished -> smape: {smape(test_serie, pred_y)*100} | mase:{mase(train_serie, test_serie, pred_y, freq)}')\n",
    "# plot_predictions(train_serie, test_serie, pred_y)\n",
    "\n",
    "#\n",
    "metrics_dict = {\n",
    "'smape_mean': np.round(np.mean(metrics_table['smape'], dtype=float), 3), \n",
    "'mase_mean':  np.round(np.mean(metrics_table['mase'], dtype=float), 3),\n",
    "#\n",
    "'smape_std':  np.round(np.std(metrics_table['smape'], dtype=float), 3),\n",
    "'mase_std':   np.round(np.std(metrics_table['mase'], dtype=float), 3),\n",
    "}\n",
    "if TRACK:\n",
    "mlflow.log_metrics(metrics_dict)\n",
    "mlflow.log_table(metrics_table, artifact_file='metrics_table')\n",
    "\n",
    "print(f'Full Pass {1+full_pass_i:5}:', end='')\n",
    "for k, v in metrics_dict.items(): print(f'      {k}: {v}', end='')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
